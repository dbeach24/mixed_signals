{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Signals\n",
    "\n",
    "Suppose we have a dataset which is known to be synthesized from two distinct signals.  One signal is of interest, while the other is not.\n",
    "\n",
    "More specifically, let's identify three distributions: $A$, $B$, and $C$, where:\n",
    "\n",
    "- $A$ is the signal of interest,\n",
    "- $B$ is the background (something we wish to ignore), and\n",
    "- $C$ is a mixture of $A$ and $B$.\n",
    "\n",
    "Given a dataset with distribution $C$, can we discover distribution $A$, while removing the influence of distribution $B$?\n",
    "\n",
    "We will examine cases where:\n",
    "\n",
    "  1. Data for $C$ is given, and\n",
    "  2. Data is given for $C$ and another dataset with an independent (but similarly distributed) $B$.\n",
    "\n",
    "For purposes of comparing our results, we will also evaluate the data distribution $A$ (i.e. the pure signal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "\n",
    "<img src=\"images/whiteboard/overview.jpg\" width=\"800\" alt=\"Overview Diagram\"/>\n",
    "\n",
    "Using the manifold assumption, we claim that data sampled from a pure signal will lie on a certain low-dimensional manifold.  Similarly, we assume that the background also lies on a low dimensional manifold, and that this manifold must be distinct from that of the signal.\n",
    "\n",
    "Assuming that our data are collected in some high-dimensional ambient space, we expect that the manifolds for pure signal and pure background will most likely occupy distinct (though not necessarily orthogonal) subspaces of the ambient space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing Model\n",
    "\n",
    "Suppose the distribution of the pure signal is represented by a multi-dimensional random variable $\\mathbf{A}$.  Similarly the pure background is represented by the random variable $\\mathbf{B}$.\n",
    "\n",
    "We employ a simple mixing model:\n",
    "\n",
    "$$ \\mathbf{C} = \\gamma \\mathbf{A} + (1 - \\gamma) \\mathbf{B} $$\n",
    "\n",
    "where the parameter $\\alpha$ determines the strength of the signal relative to the background, and $\\alpha \\in [0, 1]$.\n",
    "\n",
    "In this model, it is assumed that signal and background are drawn from independent distributions.  There is no correlation between signal and background.\n",
    "\n",
    "Using this model, we can derive the covariance structure of the mixed distribution in terms of the covariance structures of the underlying distributions:\n",
    "\n",
    "<img src=\"images/whiteboard/mixing-model-cov.jpg\" width=\"600\" alt=\"Covariance Structure\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Given:\n",
    "\n",
    "* $C \\sim \\gamma \\mathbf{A} + (1 - \\gamma) \\mathbf{B}$, and\n",
    "* $D \\sim \\mathbf{B}$.\n",
    "\n",
    "a sample dataset $C$ drawn from the mixture distribution above, we want to identify the structure of the signal manifold, by removing the influence of the background.\n",
    "\n",
    "Without further information this problem may be intractable. (How, for example, could we even generally identify what constitutes signal versus background?)  Therefore, we will also consider cases where we have a separate dataset $D$ which is an independent sample of pure background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some ideas to try\n",
    "\n",
    "### 1. Project into null space of $\\mathrm{PCA}(C, k)$.\n",
    "\n",
    "Assuming the background is known to be significantly *stronger* than the signal, we might assume that the background is responsible for most of the observed variance in $C$.  Therefore, we can perform a PCA of $C$ and project the data into the null space of the first $k$ singular vectors.  If these vectors mostly explain the background, then the remaining dimensions may better explain the signal of interest.\n",
    "\n",
    "### 2. Project into null space of $\\mathrm{PCA}(D, k)$.\n",
    "\n",
    "Instead of relying on the mixed signal $C$ to identify the background subspace, we could instead compute this from a dataset of background-only values, $D$.  By identifying the first $k$ principal components in the background, we can project the signal into a subspace orthogonal to these PCs.  Presumably the result would contain far less of the background influence, making it easier to discover the signal manifold.\n",
    "\n",
    "### 3. Using contrastive PCA, project into the subspace given by $\\mathrm{cPCA}(C, D, k)$.\n",
    "\n",
    "In the general case, the strength of the background relative to the signal will not be known.  We would like to discover a subspace which explains as much of the signal variance and as little of the background variance as possible.  Ideally, by projecting into such a subspace, we can preserve most of the signal while eliminating most of the background.  This is the subspace that Contrastive PCA is supposed to discover.  We hope to learn the manifold of the underlying signal $A$, by projecting the mixed signal $C$ into this subspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
